[
["index.html", "FlipAround Chapter 1 Welcome 1.1 Welcome message 1.2 Checklist of things to do 1.3 Pre-flight tests 1.4 MDSI Jargon", " FlipAround Authored by, and for MDSI students V:2017-11-15 Chapter 1 Welcome The purpose of this document is to ensure that all MDSI students have a source that will help to ‘wing’ the ride that is MDSI, to ensure that something exists to help the hectic ride that is Data Science and provides easy resources and easy links to information that can help you. 1.1 Welcome message MDSI is unique in its approach and feel. MDSI is a ‘boutique degree’ which means we are a small tight-knit data family which means the contacts you walk out (really) knowing are going to be more valuable than the skills you learn. In terms of content, our point of difference is the innovation in our name. We take our innovation component as seriously as data science, and is ingrained in everything that’s taught. For us, a data science degree was our innovation (we were the first of its kind in Australia), and in the rapidly changing context that is data, the ability to innovate and adapt is a pretty great point of difference for you too. Data science is a collaborative discipline. Students in the MDSI program get hands on experience of working in teams to formulate and solve real-life data science problems. Most courses focus on techniques to solve problems, but spend very little time (if any) on how problems should be formulated. The MDSI program is structured in a way that helps students learn this tacit, but crucial skill. Another important aspect of data science is that it is a rapidly evolving field. A data scientist must therefore be able to stay current with developments in the field. The MDSI program, with its emphasis on critical self-learning, prepares students to be lifelong learners. Welcome, and good luck on your MDSI journey 1.2 Checklist of things to do Getting started on your MDSI journey can be somewhat overwhelming. So to help you ease into life as an MDSI student, the following checklist will help you to get up and running as painlessly as possible. Do your statistics pre-flight test Activate UTS student email Forward UTS student email if required Enrol in your subjects Review Subject Outlines Activate and personalise CICAround Join the Slack Channel Do your CLARA profile Download R &amp; R Studio Download Tableau Activate Github Download Python &amp; Rodeo (optional) Download Rapidminer (optional) Test your Google Drive Test your Office 365 Drive Download Quantum GIS (optional) Download KNIME (optional) Log into Diigo Log into SPARK Log into Review 1.3 Pre-flight tests MDSI statistics pre-flight test: http://www.uts.edu.au/future-students/analytics-and-data-science/essential-information/mdsi-statistics-pre-flight-test 1.4 MDSI Jargon Jargon Description UTS Online UTS’s Online learning system CICAround CIC’s blog environment where students can post blogs about their studies and learning FlipAround A student guide developed by students for students in a collaborative environment CIC Squared Social networking event where students, staff and industry can connect R A coding language used in data science Python A coding language used in data science KNIME A algorithm building tool Diigo A collection of resources contributed by the MDSI community Slack A ‘chat’ type application completely driven by the student community. WordPress The host of CICAround Lynda.com An online learning platform with many free courses MOOC Massive Open Online Course Hackathon An event, typically lasting several days, where people collaborate to find information in data Azure Microsoft’s cloud computing platform AWS Amazon Web Services’ cloud computing services Coursera Website host/ service provider of MOOC’s GA Graduate Attribute CILO Course Intended Learning Outcome SLO Subject Learning Outcome "],
["the-data-science-mindset.html", "Chapter 2 The data science mindset 2.1 CLARA 2.2 Data Science Professional Competencies 2.3 Ethics and Privacy 2.4 Digital Footprint 2.5 Opportunity for overseas exchange 2.6 Electives", " Chapter 2 The data science mindset 2.1 CLARA Each person has their own learning preferences and habits of mind that shape their response to challenges and learning opportunities. CLARA (Crick LeArning for Resilient Agency) is a tool used to prompt reflection on a multidimensional construct called “Learning Power” with eight dimensions: curiosity, creativity, sense making, belonging, collaboration, hope and optimism, mindful agency and openness to change. The UTS Graduate Attributes have a strong resonance with these dimensions. The CLARA tool is used as part of MDSI activities, aiming to help the students maximise their development results through understanding themselves better, namely with regards to their approach to learning and challenges. The tool is survey-based and provides a profile-style feedback covering each of the following dimensions: Curiosity Wanting to get beneath the surface &amp; find out more Always wondering why and how Creativity Using my intuition &amp; imagination to generate new ideas &amp; knowledge Taking risks &amp; playing with ideas and artefacts to arrive at new solutions Sense making Making connections between what I already know &amp; new information &amp; experience Making meaning by linking my story, my new learning &amp; my purpose Belonging Being part of a learning community at work, at home, in education &amp; in my social networks Knowing I have social resources to draw on when I need them Collaboration Being able to work with others, to collaborate and co-generate new ideas and artefacts Being able to listen and contribute productively to a team Hope and optimism Having the optimism &amp; hope that I can learn &amp; achieve over time Having a growth mindset; believing I can generate my own new knowledge for what I need to achieve Mindful agency Taking responsibility for my own learning over time through defining my purposes, understanding and managing my feelings, Knowing how I go about learning &amp; planning my learning journey carefully Openness to change An emotional orientation of being open &amp; ready to invest in learning, having flexible self-belief, willing to persist &amp; manage any self-doubt A necessary prerequisite for developing resilience in learning Here is an example of an output from CLARA, showing the resulting profile, based on the responses provided in the survey. Learning profiles can change over time, so MDSI conducts two sets of CLARA tests, to allow the students to compare their profile changes and reflect on their development progress. CLARA survey will be organised by MDSI and each student will receive a link and instructions on how to utilise this tool. 2.2 Data Science Professional Competencies Competency can be defined as “effective application of skill, knowledge and abilities to on-the-job-behaviour and capability to perform to job requirements”. MDSI supports the ongoing development professional development efforts and offers a tool that can help the students evaluate their skills and abilities in various domains associated with the data science professional competencies. Students are encouraged to utilise the tool to identify the key competencies associated with their individual career aspirations. For each relevant competency, students evaluate their current competency levels, identify any gaps and use the information to create a set of actions that would form their professional development plan. The competencies model is introduced to students as part of 36100 (Data Science and Innovation) subject through a self-assessment exercise. The competencies are divided into two groups: technical and managerial, describing the following competency (proficiency) levels for each domain: Beginner, Competent, Advanced and Expert. Technical: Mathematics and statistics Programing and scripting languages Databases and data storage Computing systems, platforms, security, integration Data mining Data modelling Analytics, predictive modelling and machine learning Data visualisation Business analysis and interpretation Product development Interpersonal/managerial: Creativity Communication Data strategy Line management Data management and governance Facilitation and presentation Project management Competency levels: Beginner: able to assist and perform simple tasks Competent: able to perform tasks independently Advanced: able to perform complex tasks Expert: able to perform complex transformative, strategic or trans-disciplinary tasks The competencies assessment exercise aims to prompt the students to take a proactive attitude to their professional development efforts and effectively apply their analytical skills, dedication and professionalism in managing their career objectives. The competencies assessment exercise covers the following steps: Evaluate your current competency level for each domain on the list Choose a set of domains (no more than 6) that are relevant to your planned development for this subject, your course and your career goals. Identify the goal competency levels for the selected domains and describe related professional development outcomes that support your assessment Analyse your development outcomes in the context of your career goals and identify the gaps between your current and goal competency levels Develop a set of actions needed to achieve desired level of competencies and bridge the identified gap Provide feedback and suggestions for the improvement of the current list of competencies, descriptions etc. 2.3 Ethics and Privacy Its important to understand that security, privacy and ethics are three different things, although heavily intertwined in the ‘internet of things’. What is ethical when it comes to data and the internet of things? Is privacy having a login or not being identifiable as an individual? The world of Ethics and Privacy is changing, similar to the definition that now includes much more than it did a decade ago. Computer security like a login is no longer sufficient to providing protection of privacy which is more focused on ensuring that only people who should have the authority to access your information should be able to. Current Privacy legislation addresses control and authentication processes of whom can access your information via direct disclosures and how this information should be stored by the party who is collecting this information, it does not address disclosures that can be made based on inferences that can be drawn from big data of which your information is a part. Is the value or conclusions that could be drawn from information as part of big data considered private information? A sensible framework in relation to Ethics and Privacy where data is concerned is highlighted in the Belmont report which identifies two rules to consider “(1) do not harm and (2) maximize possible benefits and minimize possible harms.” A big ethical dilema of late is the rich data sources that various provider hold, that if pooled together will strip all possibility of anonymity. For more on this read: http://www.tandfonline.com/doi/full/10.1080/08900523.2014.863126?src=recsys http://libres.uncg.edu/ir/uncg/f/N_Kshetri_Big_2014.pdf 2.4 Digital Footprint Your digital footprint https://en.wikipedia.org/wiki/Digital_footprint is the name given to the data that is recorded about you all day every day. It can be the time and phone number of someone that you called, the mobile phone tower that you were connected to at the time of making the call and how long you spoke for. It is the IP address of your computer when you connect to the internet. It is the list of items you pay for when you go through the checkout at the supermarket and the eftpos card number you used to pay for the items. It is the surveillance footage you appear in when you move through monitored public spaces. It is stories you ‘like’ or share on social media sites. It is the journeys that your GPS navigation stores about your travels. It is every email you send and every click you make when you browse the internet. Your digital footprint is the inescapable record of your existence by doing nothing more than living your life. It is an important aspect of modern society as many services that you enjoy depend on the data you generate in order to provide critical services. A bank can’t tell you how much money you have without keeping record of your bank transactions. For good or for evil, this data comes embedded with far more information about you. By looking at the kinds of things you spend your money on or the businesses that you spend your money at and the time of day that you spend your money there, it can be determined where you live and where you work. As an MDSI student, you will learn to think critically and ethically about data collection and how it can be used for good and for evil. The best place to start your thinking is with your own digital footprint, become aware of how big it is and how you feel about it. It’s important to note that very little permission is sought on data collection and when it is sought, very little education is provided in terms of the use of that data. Very few providers who collect data clarify or specify what the data they collect is used for. You are responsible for your digital footprint. Generate it wisely. 2.5 Opportunity for overseas exchange Some great opportunities exist within MDSI with our Program Director having many contacts in many other countries which enable us to be able to explore greater opportunities for overseas exchange. You need to do a few things before this opportunity is explored as set out by the Program Director to ensure for an easier way forward if this is an opportunity you want to explore. 2.6 Electives You need to select four electives during your MDSI course. These electives should be selected to assist you in your growth as a student and as a data science professional. These subjects enable you to add to your toolbox of where you are heading with your journey. Electives can be selected from any school however you will still be subjected to the pre-requisites for any possible subject, so it will depend on the requirements of the subject. We suggest that when you apply for a subject with a prerequisite that you also apply for a waive of the prerequisite if the prerequisite is a subject you are familiar with but have not done with UTS and get exemption for that requisite. This is not always easy, or approved and is subject to each School’s internal views or policies. It is definitely a consideration to take. You can apply for the subject ( and a waiver of prerequisites if required) early as CIC is not limited by inter-faculty time restrictions. Our best tip is : get in early. "],
["a-survival-guide-to-mdsi.html", "Chapter 3 A ‘survival guide’ to MDSI 3.1 First steps 3.2 Technology 3.3 Writing 3.4 Research &amp; Library Access 3.5 Professional Experience 3.6 Data Security 3.7 Hackathons 3.8 Cloud Computing 3.9 The art of ‘self learning’ 3.10 Resources and learning", " Chapter 3 A ‘survival guide’ to MDSI 3.1 First steps 3.1.1 Your UTS email: First and foremost you need to activate your UTS email address. All official communications from UTS, subject notifications, MDSI newsletters etc will be sent to this email address. You need to activate your email address before you can access other UTS systems. Activate your UTS Student email: Navigate to https://email.itd.uts.edu.au/webapps/myaccount/activation/ and follow the steps to activate your UTS student email account. ** Protip: ** If you don’t want to login frequently to check if you have mail, simply setup a email forwarding to an email address of your choice via the settings page after logging in. For more general information about using UTS systems go to: http://www.uts.edu.au/current-students/managing-your-course/using-uts-systems/uts-student-account 3.1.2 Enrol in your subjects: It is really important that you check your enrolment. If you are not enrolled, you cannot participate in your studies. This might seem obvious, however at UTS you need to do more than simply accept your offer. Once you have received and accepted your offer to study, you then need to enrol into your subjects. If you have not enrolled, you need to login into the My Student Admin https://onestopadmin.uts.edu.au/estudent/Login.aspx portal to enrol. For more information about enrollment and a step by step instruction guide, please visit: http://www.uts.edu.au/current-students/managing-your-course/your-enrolment/how-enrol 3.1.3 Get your subject outlines: MDSI uses a variety of systems for online teaching and learning. UTSOnline https://online.uts.edu.au/ and CICAround https://ca.uts.edu.au are the two primary environments for you to familiarise yourself with. The first thing you need to do after activating your email address is to login to UTSOnline, access your subjects and find your subject outline. Your subject outline contains everything you need to know about your subject for the coming semester. It includes the contact information for your subject co-ordinator, important dates, assessment descriptions and much more. In most cases you can find the answer to any question you might have about your subject addressed in the subject outline. Find your subject outlines in UTSOnline: Login to UTSOnline at https://online.uts.edu.au/ using your student ID number and the password you setup for your UTS email account. Access your subjects by clicking on your subject name Download your subject outline by clicking on the link titled ‘Subject Outline’ in the left side menu, then click on the subject outline link on the page. 3.1.4 Join the MDSI community: Your next stop should be CICAround. Here you will connect with your peers in an academic capacity. There are discussion forums for your subjects where you can post questions. CICAround most notably is where you will go to blog about your experiences throughout your MDSI journey. The first step is to activate your blog. Then you can browse through the blogs of your new MDSI family and read about their experiences and the things they have learnt. Activate and personalise your CICAround profile: Navigate to https://ca.uts.edu.au/using-ca/ Watch the welcome video then login to CICAround using your student ID and password. Put up your first CICAround Blog post 3.1.5 Join the MDSI chatter: Slack has proven to be a very useful tool so far. It is completely driven by the student community and is where the MDSI student community goes to socialise, organise BBQs, ask each other for technical help and share useful resources. If you need a quick answer, Slack is the place to go. Join the Slack Channel You can download the Slack application from https://slack.com/downloads You can also get the app for IOS, Android and Windows phones. You do not need to pay for a subscription. Sign Up to the MDSI group at: https://utsmdsi.slack.com/ If you’re completely new to Slack, there are some helpful getting started guides at https://get.slack.help/hc/en-us/categories/202622877-Slack-Guides 3.2 Technology 3.2.1 R / R Studio ‘R’ is a coding language used by most of the data science community. RStudio is a software program or ‘Integrated Development Environment’ (IDE) that makes working with the R language ALOT easier. The programming environment is really flexible as it allows you the joy of working in a notebook format, scripting, markdown and publishing your work as a PDF. You will use R in many of your subjects and being able to use it well will give you a serious edge over your classmates and competitors at hackathons. Download and install R &amp; RStudio free Download and install the R language: https://cran.rstudio.com/ Download and install RStudio IDE: https://www.rstudio.com/products/rstudio/download/ Libraries well worth their weight in gold: tidyverse http://tidyverse.org/ A collection of libraries that make data analysis easier readr http://readr.tidyverse.org/ for reading all kinds of data formats stringr http://stringr.tidyverse.org/ for working with text ggplot2 http://ggplot2.tidyverse.org/ for visualising data tidyr http://tidyr.tidyverse.org/ for creating tidy data dplyr https://github.com/hadley/dplyr for manipulating data caret http://topepo.github.io/caret/index.html for creating predictive models Bookdown https://bookdown.org/ for creating beautiful documents There are many resources to get you started in doing data science with R. Refer to the resources section for more information. 3.2.2 Tableau Tableau is a tool for visualising data. It is quite powerful in its ability to connect to a variety of data sources both on your computer and through the internet. It is also relatively intuitive to use. As a student you can apply to the company for a free license to their commercial desktop version. https://www.tableau.com/academic/students 3.2.3 Github Github is a fantastic tool to become familiar with. It is a great place to store code, collaborate with others and even host your own website or blog. Github has a really generous collection of free stuff for students. To claim yours head over to: https://education.github.com/pack 3.2.4 Python / Rodeo / Jupyter Notebook Python is a general purpose coding language widely used by the data science community. A great place to start is with Anaconda from Continuum Analytics : https://www.continuum.io/downloads Anaconda comes with a ‘container’ management environment called ‘conda’ and ships with a collection of scientific python libraries that have optimised for fast computation. It is also really helpful to manage your libraries and will let you know if there are incompatibilities between the libraries you are using. This is just the tip of the Anaconda iceberg. Python for data science is commonly used in a notebook format. To this end Jupyter notebooks will become a familiar friend. Fortunately it is included as part of Anaconda. For more info, refer to the resources section. If you prefer an ‘R’ style IDE, then Rodeo by Yhat is for you. https://www.yhat.com/products/rodeo If you prefer a traditional IDE, you can get a free license for PyCharm (as well as all their other products) from JetBrains using your student details: https://www.jetbrains.com/student/ Libraries well worth their weight in gold: Numpy http://www.numpy.org/ for working with numerical arrays Scipy https://www.scipy.org/scipylib/index.html for scientific computing with python Matplotlib http://matplotlib.org/ for visualising data Seaborn http://seaborn.pydata.org/index.html for statistical visualisation Pandas http://pandas.pydata.org/ for working with data Statsmodels http://www.statsmodels.org/stable/index.html for creating statistical models Scikit-Learn http://scikit-learn.org/stable/ for doing machine learning with python Tensorflow https://www.tensorflow.org/ ‘deep learning’ with python Note: Python comes in two different flavours: 2.7 and 3.x. You can start with either version, but it is worth learning what the subtle differences are (eventually). A couple of blog posts to help you choose between R and Python: R vs Python for Data Science: The Winner is http://www.kdnuggets.com/2015/05/r-vs-python-data-science.html R vs Python for Data Science: Summary of Modern Advances https://elitedatascience.com/r-vs-python-for-data-science 3.2.5 KNIME KNIME Analytics Platform is an open source solution that enables quick, fast data driven designs for machine learning. Its a visual tool to learn and use when you need to get the job done quickly (without writing any code) and need to create algorithms quickly but you don’t have the time to learn the mathematics behind the algorithms. Its friendly and easy to use to find the hidden ‘story’ in the data. Go to https://www.knime.org/knime-analytics-platform to download KNIME for free. 3.2.6 Rapidminer Rapidminer is another visual tool for doing data analysis, modelling and machine learning. You can get access to their commercial tools using your student status from https://rapidminer.com/educational-program/ 3.2.7 Quantum GIS QGIS is a really nice open source tool for working with geospatial data. To get started just head over to http://www.qgis.org/en/site/index.html 3.2.8 Diigo A collection of resources contributed by the MDSI community. Join the Diigo group - simply create a Diigo account and request access. https://groups.diigo.com/group/cic_mdsi Frequently used search tags include: “DSI, DAM, Data, big_data, case studies, visualization, teaching_tools, statistics, stats-thnkg, privacy, Algorithms, ethicsVSD, realworldDM, video, Analytics, human-machine, history, data,mining, Data_science, cisco, R, IoE, TEDtalks, values, QSProject, Algorithmic, Accountability, industry, sociotechnical, systems, AI, podcast, professional_practice, portfolio, storytelling, bbc, QS, DMonline, innovation, humanismprofessional, development, open_data, speculative_futures, RealWorld, ted, transdisciplinarity, creativity, algorithm, sociotechnical, BowkerStar, futuresgender, challenge, data-sets, accountability, digital_futures, tools, DM, reading, DVN, equality, infographic” 3.2.9 Google / Office 365 Your university account allows you access to Google Apps and Office 365. Google Apps access does not include Gmail. You can not login to your UTS google apps account via gmail or if you are already logged in with a personal gmail account, you will need to log out completely from gmail. Once you have done this, you can log in using your student email address. This will revert you to a UTS login page. Use your UTS student number and password and it will revert you back to the Google Drive, but you will be logged into the drive. Similarly you can mimic the same steps for Office 365. 3.2.10 SPARK-Plus SPARK is an acronym for Self and Peer Assessment Review Kit. This tool has been developed to assist with in class activities as well as being able to self and peer assess assignments. Its also one of the few tools that make group assessments/work easier, in particular to marking. Login with your student ID and Password https://uts.sparkplus.com.au/login.php 3.2.11 Review Review is an assessment tool that is used to mark your work, give you feedback about your work and for you to develop a sense of what is expected by marking your own work before it is assessed by teaching staff. Review allows you to see feedback from your lecturer as well as your mark broken down by specific GA/CILO set out by your assignment. It also allows you to self assess your assignment, which allows the lecturer to see if your expectation is in line with their expectation. It’s important to note that the lecturer will not be able to see your self assessment until after they have saved your mark. You are also able to see the average of the class as well as where the staff has measure you. https://uts.review-edu.com/uts/ 3.3 Writing 3.3.1 Blogs More and more academics and workplaces use blog posts to reach clients, audiences and share knowledge. Blogs can be useful for many reasons and is used as a reflective tool for students as well as providing an opportunity to share any learning. You can use some tools to turn topics into amazing titles by using keyword suggesters (http://keywordtool.io), title generators ( https://www.portent.com/tools/title-maker), and you can also test your headlines with the following tool (http://coschedule.com/headline-analyzer#) Tips for new bloggers Use an eye catching title In-text links Use pictures, pictures speak a thousand words Keep post to 1000-1500 words Use social sharing buttons Use paragraphs - one idea per paragraph Revise and Rewrite Omit needless words - Use the KISS (Keep It Simple, Stupid) Principle Use definite, specific concrete language - direct and to the point Write in a way that comes naturally - use your active voice Be clear - make it simple to read and understand Avoid fancy words Do not take shortcuts at the cost of clarity Tips on writing blog posts : https://problogger.com/how-to-write-great-blog-content/ and http://www.socialmediaexaminer.com/26-tips-for-writing-great-blog-posts/ 3.3.2 White papers Where do you start with a white paper and what are they? White papers are originally documents written for government policies however this is most recently being used by companies and universities to get new policies and research into the public space. There are some things to consider when writing a white paper: Pick a topic people will want to read or a problem you want to solve Pick a generic title that describes the problem at hand Engage, inform and convince your reader Be descriptive and professional Consider the audience you are ‘speaking’ to and accommodate for their level of expertise Set up a great intro Emphasize the value you want to or will create Decide on a length for the white paper (1-5 pages are the norm) Describe the solution you are proposing Remember a summary that reviews the problem, solution and result of the outcome Proofread your document, and ensure someone else reads it before you submit/publish it. Follow the 3-30-3 rule ( you have three seconds to captures your audience’s attention from a glance at your piece, if you succeed at capturing their attention then you have 30 more seconds to ensure they continue reading, if you pass the 3-30 rules then your reader will give you three more minutes to make your point). If you would like to enhance your academic writing skills, you might be interested in have a look at the Academic Phrase Bank: http://www.phrasebank.manchester.ac.uk/ 3.3.3 Assessments The majority of your information regarding a subject and the assessments is contained in your subject outline. This is your base document and you should follow it closely. In addition you will get an assignment brief for each assignment you have due. Its is recommended that you review these briefs and that you follow the detailed instructions set out for you. 3.4 Research &amp; Library Access Research is something you will do a lot throughout your studies. There are many contexts that will shape the way you research. For the purposes of finding academic peer reviewed sources, some tips below will likely come in very handy. The Library ( http://www.lib.uts.edu.au) has great resources, workshops and tools that will help you during your education if you choose to utilise them. If you need some help understanding how to use the library, they have produced some short videos to help you get started: http://www.lib.uts.edu.au/headsup The library website has an entire section dedicated to research: http://www.lib.uts.edu.au/research There is also some self paced training modules you can do to help you get the most of what the library can offer for your research. http://www.lib.uts.edu.au/headsup-researchers 3.4.1 Searching the catalogue One of the great things about the library catalogue is that it returns results far beyond the resources held by the library itself. UTS pay subscription fees to many 3rd party resources including journals, publishers and more. The best thing about this is that if you find something that has an online source available, you will likely be able to download a copy to your computer for later reading. As an example, the link below will take you some search results for the term “Data science” and was then filtered to only show ‘online’ resources. http://find.lib.uts.edu.au/search?Ntx=matchallpartial&amp;Ntk=All&amp;N=4294967183&amp;Ntt=data%20science If you click on the ‘Available’ link underneath each resource, it will then offer you the option of launching the electronic resource. 3.4.2 Databases &amp; Articles If you prefer to search specific sources, you can browse the databases section for 3rd party providers. This includes sources that UTS subscriptions that allow you access as a student. Similarly if you wish to focus your search for specific journal articles rather than journals or books, the ‘Articles’ button is a great place to go. 3.4.3 Referencing It is really important that you get used to referencing from the start. UTS uses the Harvard Referencing style. Fortunately there is an interactive referencing guide available through the library to make things easier: http://www.lib.uts.edu.au/help/referencing Make sure to browse through some of the other links at the above link. You might find some other useful tips (tools). 3.4.4 Library events &amp; tours The library team will help you wherever they can. It is recommended that you keep an eye out for all their events on http://www.lib.uts.edu.au/events The following events, particularly for MDSI students, are coming up in the next month and below are links where you can register. MDSI: Data Science Research and Referencing Tue, 14 March, 2017 10:00 AM to 11:30 AM This workshop is a practical introduction to advanced research skills and reference management tools, with a focus on data science. Laptops are recommended but not essential. Participation is open to MDSI students. A concurrent workshop will run if more than 24 participants register for this session. http://www.lib.uts.edu.au/event/609872/mdsi-data-science-research-and-referencing MDSI: Library Tour and Scavenger Hunt Tue, 14 March, 2017 6:00 PM to 7:00 PM An interactive tour (it’s a scavenger hunt) of the UTS Library services and facilities for MDSI students. Meet in the Library foyer. https://www.lib.uts.edu.au/event/609880/mdsi-library-tour-and-scavenger-hunt 3.5 Professional Experience At times Industry will approach CIC for students who might be interested in internships. These are posted by the CIC:MDSI team in CICAround whenever these possibilities comes up. You can find the postings on CICAround (https://ca.uts.edu.au/blog/) 3.6 Data Security Include a security/privacy sub section (must cover concepts of masking, encrypting etc data) Data security is defined as measures that is used to protect digital privacy as to prevent unauthorised access to computers, databases, websites and other digital items. It also protects data from corruption. Data security can include backups, data masking, encryption or even data erasure. Data masking is defined as the process of changing certain part of data so that the structure remains the same but the information itself is changed to protect sensitive information. It ensures that sensitive information is unavailable beyond the permitted environment. It ensures that the original values are not re-engineered or identified. Eg is user training and software testing. Data encryption ensures that data is unreadable to users who are not authorised to access the data and who do not have the ‘key’. One of the most common ways of securing data is using authentication like passwords, and other data that can verify an identity ( like email and password login) prior to granting access to a system. These measures are taken to ensure hackers that use alternative system access methods to sabotage computer systems and networks. Hacker actions can be illegal or legal depending on the purpose behind the actions. There are three categories of hackers : Black hat hackers break into computer systems illegally and cause harm by stealing or destroying data, i.e., a banking system to steal money for personal gain. White hat hackers use their skills to help enterprises create robust computer systems. Grey hat hackers perform illegal hacking activities to show off their skills, rather than to achieve personal gain. 10 General Data Security Tips: Back up early and often Use file-level and share-level security Password protect documents Use EFS encryption Use disk encryption Use a public key infrastructure Hide data Protect data in transit Secure wireless transmission Use management or access control Types of Encryption: Triple DES : It uses three individual keys with 56 bits each that adds up to 168 bits. This is a dependable hardware encryption solution RSA : A public-key encryption algorithm and is standard for encrypting data over the internet. Is a asymmetric algorithm due to use of a pair of keys. There is a public key, to encrypt, and a private key, used to decrypt. Blowfish : the symmetric cipher splits messages into blocks of 64 bits and encrypts them individually. Its high speed and very effective. It’s also free source software. Twofish : Symmetric algorithm with keys up to 256 bits, only one key is needed. The fastest of its kind and ideal for hardware and software. It’s also free source software. AES : Most trusted algorithm by U.S. Government. Has an efficient 128-bit but also a 192 and 256 bit algorithms for heavy duty encryption purposes. Considered impervious to attacks except brute force. Honey Encryption : deters hackers by serving fake data for every incorrect guess. It slows down attackers but also provides a haystack of false hopes and makes it difficult for hackers to find the correct key. http://www.computerworld.com/article/2546352/data-center/top-10-ways-to-secure-your-stored-data.html http://www8.hp.com/us/en/software-solutions/what-is/data-security.html http://www.lexisnexis.com.au/en-au/products/privacy-confidentiality-and-data-security.page 3.7 Hackathons Hackathons are competitions (socially or sometimes for a prize) that challenge you with a goal or a problem. In a data science context this typically involves datasets and your wits against a clock. Hackathons are a fantastic way to learn from each other, to ideate, validate, develop your skills and sometimes even build a prototype. Hackathons are one of the most authentic learning experiences you can have as a data science student. You will practice all sorts of skills you need to become amazing: Team work How to frame a problem Data investigation Practicing and learning all kinds of technical skills Data storytelling Presentation &amp; selling your ideas Networking Hackathons are educational, engaging and empowering. You do not need to feel ready before you participate. The only thing you need to do is show up, have a positive ‘can do’ attitude and have fun. They last anything from a few hours to a few days. MDSI students have been leaving their mark at these events by taking home the prizes as well as the really big prize checks as can be seen on display in CIC. The most popular one to get involved in is ‘Unearthed’: http://unearthed.solutions/ Our very own ‘Data Cake’ took home the first prize in 2016, ‘Perry’s Fan Club’ took home shared second prize in 2016 as well as ‘Team Beaver’ taking home Young Innovator Award. If you don’t want to wait for an event and want to sink your teeth into a hackathon right now, you can participate in online data science competitions. Here are a few links to get your started: Kaggle https://www.kaggle.com/ DrivenData https://www.drivendata.org InnoCentive https://www.innocentive.com/ar/challenge/browse A list of hackathons updated weekly : http://disruptorshandbook.com/big-list-hackathons/ Another good source of hackathons : http://www.hackathonsaustralia.com/ 3.8 Cloud Computing Cloud computing is a phrase used to describe the act of doing things that you could do on your own computer but doing it on a remote computer instead. There several reasons why you might choose to do this. In the context of data science, this is usually to take advantage of ‘compute clusters’. Compute clusters are exactly what it sounds like. A cluster of computer processors are used together to give you the advantage of their combined power which leads to faster data processing. This makes it possible to run analysis routines on large datasets (gigabytes, terabytes or even petabytes) that could take days or weeks to run on your laptop, in mere minutes or hours. There are a few more concepts to understand, but essentially the bottom line is that cloud computing makes things faster and big datasets more accessible from an analysis point of view. There are several providers that operate in this space for data science purposes. Some of these include: Amazon Web Services (AWS) https://aws.amazon.com/big-data/?nc2=h_l3_bh Microsoft Azure https://azure.microsoft.com/en-us/services/machine-learning/ Google Cloud Platform https://cloud.google.com/ Make sure you claim your $100 student credit when you sign up to the Github student pack https://education.github.com/pack 3.9 The art of ‘self learning’ Self learning is the process of teaching ‘self’. With the boom in freely available technology it has become possible for each student to dictate their own learning experience and how much they want to expand. They can either learn the minimum of what is being taught in class, or they can put in extra time and work and participate in the exciting adventure of self learning. This involves using various resources from YouTube, Journals, Additional Books and anything else you can get your hands on that adds to your skillset and your knowledge base. In recent days self learning is considered an art with lecturers there as guidance or mentors on this journey where you can develop your skills to great depth in a short amount of time. There are a few things to keep in mind during this journey : Reputable sources for learning a concept is required. If you plan to embark on the art of self learning you need to make sure that the sources you are learning from, know what they are talking about. What can be considered reputable sources? Academic journals or white papers Non- biased sources Generally, personal blogs are avoided. However, in this constantly updating space we might need to learn from other expert in the field that explain these fast changing concepts. Read textbooks and books about the topic. A large collection of books are available to you as a student for free via the Library (you can sometimes even download a pdf version for your personal use). Watch the videos on YouTube on the topic, especially for the more tricky, hands on or mathematical based subjects. Learn from your surroundings. Never underestimate the value others can bring to your journey. This ranges from mentors to other students, some students are further in their journey and can help you along. There are also various meet-ups and networking events surrounding these topics. Hands-on experience is by far the best. Join a hackathon, or get an internship. Use guided learning experiences, like MOOC’s on Coursera or a range of other platforms. Plan your success. Write down your goals, learning outcomes and what you would like to master or be able to do and endeavour to move toward those goals (this will be really handy in iLab too). 3.10 Resources and learning Online short courses: Lynda.com : you have free membership via the Library which gives you access to all the courses, https://www.lib.uts.edu.au/goto?url=https://shib.lynda.com/Shibboleth.sso/InCommon?providerId=https://aaf-login.uts.edu.au/idp/shibboleth Coursera : membership is free, https://www.coursera.org/ DataCamp : you have free membership via MDSI which gives you access to all the courses, https://www.datacamp.com/, once you join ask to join MDSI group and the courses open up. Online Books: R for Data Science by Garrett Grolemund and Hadley Wickham - http://r4ds.had.co.nz/ Applied Predictive Modelling by Max Kuhn - available via the library https://link-springer-com.ezproxy.lib.uts.edu.au/book/10.1007%2F978-1-4614-6849-3 (requires login with yoru student ID and password) Python Data Science by Jake VanderPlas https://github.com/jakevdp/PythonDataScienceHandbook Github repositories with great links: Free programming books https://github.com/vhf/free-programming-books/blob/master/free-programming-books.md#python Python Data Science tutorials: https://github.com/ujjwalkarn/DataSciencePython R Data Science tutorials: https://github.com/ujjwalkarn/DataScienceR Machine learning &amp; Deep learning tutorials: https://github.com/ujjwalkarn/Machine-Learning-Tutorials/blob/master/README.md Some more Github resources and corners of the web to explore: https://www.analyticsvidhya.com/blog/2016/09/most-active-data-scientists-free-books-notebooks-tutorials-on-github/ "],
["data-futures.html", "Chapter 4 Data Futures 4.1 Andrew: On Visualisations 4.2 Corinna: ON SELF-SERVE ANALYTICS AND DATA DEMOCRATISATION 4.3 Herry 4.4 Passiona 4.5 Rory 4.6 Tracy", " Chapter 4 Data Futures 4.1 Andrew: On Visualisations 4.1.1 True Value and Purpose of Visualisations To consider trends in visualisation let’s begin by identifying the core value of data visualisation - what does it do? In general terms we think about things using language but we understand things using imagery. Our memories work better when we link items to images (even unrelated images) as evidenced by many mnemonic tools. Data visualisation is the process of presenting information using means other than language as the principal conduit for transfer of meaning/understanding/knowledge. If we think about our senses, 3 of the 5 (touch, smell, taste) are principally involved with our situational awareness. Hearing and sight, whilst obviously having key situational awareness roles, are our principal sources of knowledge awareness/learning at a higher cognitive level. Higher level information that we receive verbally is principally delivered via language - yes we could be learning about sounds themselves in which case there is a combination of non-verbal and verbal but when attending lectures or at work listening to the boss the language is important - doubtless we are also processing a ton of non-verbal messages at work too. At uni and at work our non-situational visible inputs are often computer screens or paper - what Edward Tufte refers to as “flatland”. So now consider what we look at in flatland - a lot of words and numbers which require processing via our brain’s language centres (Wernicke’s area, Angular Gyrus, Insular Cortex, etc.) before they can be understood. That understanding is very often visual, do you see what I mean? Data visualisations cut out the language middle-man and, because they are not constrained by the bandwidth of our language processing systems, provide a high-speed information channel capable of carrying lots of data very quickly into the “understanding” part of the brain. Language is an incredibly powerful brain function. It is arguably that which lifts humans above other species, more even than opposable thumbs, but it is slow to process compared to image processing - we have been finding meaning in what we see for a lot longer than we have been translating meaning into and out of language. All this is by way of suggesting that the purpose of data visualisation is to provide a means to convey understanding/knowledge without the use of language. In practice, of course, language is often used to augment/enhance visualisations (scales, labels, titles, explanatory notes) but in many cases the data is just way too dense to be conveyed in any other manner than a visualisation (think picture, thousand words) as evidenced by the most common, oldest and data dense visualisations we have - maps. Figure 4.1: School map of the Canton of Zurich 1:150 000, Eduard Imhof and collaborator 4.1.2 The Effective Use of Visualisations This chapter is heavily influenced (as many discussions of visualisation are) by the works of Edward Tufte, the Yoda of data visualisation. Yoda is an appropriate term because his approach is to use many examples of good and bad data visualisation practice and his objective appears to be to guide and advise by providing clarity as to why certain visualisations are easier to understand. He advocates concepts such as: minimise use of non-data ink remove chartjunk avoid harsh palettes its okay to have high data density Perhaps because of the clarity and almost pervasive uptake of Tufte’s guidelines/advice, we are beginning to see more appropriate (gentler) palettes being offered as default colour schemes and more awareness of perception issues (e.g. Moirè effect) in business intelligence tools such as Tableau and Sisense. Figure 4.2: Moirè Effect - Vibrations in the Image Before diving into trends and fads though, pause to consider and remember that the objective of data visualisations should be to transfer knowledge and understanding via images rather than words. If we understand things ourselves in a visual way and want to transfer that understanding to our reader then we can most simply do that by presenting that information visually. Are you beginning to understand why all those styles of the 90’s with Moirè effects were so bad? The effective use of a visualisation to transfer knowledge or information, however, assumes there is knowledge/understanding on the part of the author in the first place. As we turn to tools, technologies, trends and fads consider the risks associated with any mechanism that makes it easier to deliver “information” about data (especially big data) even if the “author” doesn’t understand the data themselves. 4.1.3 Tools, Technologies, Trends and Fads The first, and possibly most significant, trend of data visualisation is the need for it. Big data is presenting a challenge. A lot of information is being gathered and supporting a conclusion or recommendation based on big data often requires some form of supporting encapsulation/presentation of that information. Overly simplifying large volumes of data risks losing the message behind the detail so visualisation is becoming a more important tool. The volume and depth of data, the need to present multivariate analyses, the complexity of the messages all can be addressed by powerful, well-structured data visualisations. 4.1.3.1 How Big is Yours? In 1990 Tufte mused on: the essential dilemma of a computer display: at every screen there are two powerful information-processing capabilities, human and computer. Yet all communication between the two must pass the low-resolution, narrow-band video display terminal, which chokes off fast, precise, and complex communication. (p.89, Envisioning Information, 1990, Edward Tufte) Clearly, screen resolutions have progressed very significantly since 1990 such that HD displays (e.g. Retina) are providing resolution and colour/contrast ranges much closer to the range of the human eye to perceive. To this we are now adding virtual reality, data walls and data rooms. All of these are permutations of “bigger display spaces” to display more data. Its cool and sexy and great for University PR but does it work? (Doubtless this risks being very unpopular with certain groups at UTS) Does the human mind have the capacity to work in the round, holding context from one part of the room to the next? We have trouble maintaining data context flipping a page so I’ll let you decide how easily we maintain that context turning around. So what does that mean for the super-sexy, very expensive displays - use them wisely. They are not without merit or purpose but just throwing stuff up there to show off risks falling into a world that Tufte might describe as “mega chart junk land”. Figure 4.3: Mine’s Bigger… There is an important philosophical point to be made here. If you really need a 360° view of your data to explain it - do you actually understand it that well? It goes back to the underlying requirement that the author understand the message before trying to convey it. With that said, these new display technologies are understandably being played with, people are learning what can be done and how to do it - hopefully as part of that they will also learn if/when they actually need 360° views (VR, data rooms). Don’t let the medium get bigger than the message. 4.1.3.2 Intelligent Analytics - the Good, the Bad and the Less Ugly Looking at the array of smart business intelligence tools being brought to market (e.g. Tableau, Sisense, Periscope) it is clear that the integration of data access with data analytics is progressing at pace with a big component of these visualisation tools being their libraries of data access layers and data wrangling tools. This work is directed at removing the peripheral effort from the work involved with analysing and presenting information about data. Visualisation tools are being built/interfaced into stream processing solutions/environments like AWS Kinesis to provide real time data visualisations more simply. Open source tools like Kibana and Elasticsearch are providing accessible functionality while R and Python extensions like Shiny apps, ipywidgets and Bokeh plots are providing similar (if less readily accessible) functionality in the machine learning community space. The power of the commercial tools to provide tips, advice and suggestions is one of their big commercial selling points and if they are being used for data investigation then this is not a bad thing. It doubtless streamlines the process of investigating and understanding data. If used for this purpose this sort of guided analytics is useful but care needs to be taken that the guidance doesn’t limit the analyst’s ability/willingness to do the hard work to understand all the possible interpretations of the data they are viewing. Worse still, the easier it is to produce good looking visualisations, the greater the temptation/capacity to present data that is not truly understood by the “author” of the visualisation. Figure 4.4: Simple Really A scan of the various (newer) data visualisation tools shows an increasing awareness of how to present data (the less ugly). Being a commercial marketplace there remains (at least in the demo side) a preponderance of the gaudy, flashy visualisations but that is to be expected. The move to the pastel palettes, fainter lines, less cluttered, confronting visuals are all improvements on the clunky, flashy graphics of the 90’s and 2000’s but the balance between understanding the data (the message) and the best way to present it so as to be easily consumed by its observer (the medium) remains a challenge. This is not least because there are a lot of skills required for both those tasks. Thus, data analytics is moving forward in: improving access to source data improving exposure of data/information (interactive visualisations, live visualisations) tentative steps to use new display technologies better physical (perceptual) properties of visualisations 4.1.3.3 Recommendations So what are the best (or least worse) tools options available today? There really is no single recommendation that can be made but the questions to be considered might guide any selection process: What is your objective? Where is your data? How much access and wrangling is required? Is your data real-time? What are your circumstances (corporate budget, personal or research)? What are you priorities (analysis, presentation)? What are your legacy technical and political constraints? A wily consultant would read this as an opportunity in and of itself. The tools are less the issue than the desired outcome. Visualisations make data more accessible - large volumes of data can be represented in a small amount of space, data can be presented in an approachable manner to colleagues, customers and the public. The adoption of data visualisation as an integral part of how information is presented rather than as a fad or gimmick is progressing apace so if you read some of Tufte’s work or that of predecessors and more recent data visualisation advocates and bear the advice in mind when building data visualisations then that is the best start that can be made. 4.1.4 The Future of Visualisations So, more data, from more sources is being amalgamated more effectively. Businesses and researchers are looking for ways to better transfer understanding of the information buried in those large, complex data sets. What might happen next? The gimmicky/fad nature of some of the display formats (VR, data wall, etc.) will diminish with the combination of a better understanding of their applied values and improvements in tools that utilise the capacities of these technologies. This probably isn’t the next big step in data visualisations, however. The big steps happen when components that already exist are integrated more effectively. There is a slow move away from traditional reports-based business operations to dashboards and real-time awareness of the state of a business, market, campaign, etc. The nirvana of business intelligence is to amalgamate all the disparate data sources available to a business in a manner that exposes the underlying forces/reasons behind business trends. The data access tools are facilitating the move towards this nirvana. Once the data is exposed then the next two steps will be: Use data visualisations as real-time views (or historical real-time views) of the state of a business (move away from tables and reports). Provide visualisation interfaces (now we get to Minority Report territory) linked to business models allowing business planners to see the impacts of different business decisions and strategies. As to the next area of advanced visualisation research, perhaps image processing neural networks can be used as a starting point to reverse engineer the principal visual vectors of understanding. 4.1.5 Bibliography Imhof and collaborator 1969, Schulkarte des Kantons Zürich 1:150 000 Orell Füssli AG, Zürich. Tufte, Edward 2001, The Visual Display of Quantitactie Information, Graphics Press Tufte, Edward 1990, Envisioning Information, Graphics Press 4.2 Corinna: ON SELF-SERVE ANALYTICS AND DATA DEMOCRATISATION The ability to enable evidence based and data driven decision making in the workplace has been evolving in the data science industry through the introduction of self-service data analytics tools. Speculators such as Gartner predict that by 2020, self-service business intelligence applications will make up 80% of all enterprise reporting (Dykes 2016). Self-service analytics seeks to provide novice analysts and non-technical business users with the capability to access and use or manipulate data with the hope that these activities will lead to new knowledge (Rouse 2014). Vendors including Tableau, Power BI and Hyper Anna, amongst others are investing into this trend by combining artificial intelligence solutions or user interfaces to manage the thirst of non-technical users for data. Opening up analytics to wider audiences promises to develop new insights, knowledge and innovation by crowdsourcing minds (Kitchin 2014). Therefore, self-service analytical tools are enablers to the effects of data democratisation, breaking down data silos, and providing access to data when and where it is needed at any given moment (Kitchin 2014). The push for evidence-based thinking in the workplace is justified by a legacy of successful outcomes of this approach in many industries especially medicine. The medical field provides a golden standard of an area where evidence based decision making is clearly valuable. Historically, medicine has relied on funded random controlled trials and other forms of formal research to develop standards for decision making; favouring treatments which had been proven to be most effective in practice (Lumen 2017). By relying on data driven evidence, much of the uncertainty about treatment practices has been removed, further improving the quality of services. Primarily evidence based practices aim to improve the quality of decision making by justifying actions and applying knowledge derived from data. The flow of knowledge development can be represented by the knowledge pyramid below, where data are first abstracted from the world before being processed and repackaged into usable artefacts (Kitchin 2014). Figure 4.5: A Knowledge Pyramid (taken from Kitchin 2014 and adapted from Adler 1986 and McCandless 2010) Traditionally, due to the costs and constraints of generating data, the practices of generating new knowledge with data has been constrained to larger entities that could afford the funding and personnel required (Kitchin 2014, Bowker 2000). Smaller amounts of data were formally collected in studies designed with established methodologies and modes of analysis, as well as rules of conduct (Kitchin 2014, Bowker 2000). There is a long reaching record of producing answers to tailored and specific research questions and iteration on the scientific process. As phenomena become easier to monitor with the help of the digitisation of data, the amount of data, technologies and techniques available become more accessible at a lower cost to time and effort (Brynjolfsson &amp; McAfee 2014). Concerns have arisen over the risks of misinterpretation, and misuse of information generated in the data by untrained users (Marr 2017, Dykes 2016 and Harris 2012). 4.2.0.1 ON BIAS Untrained users may be unaware of and unable to assert control over personal biases. There have been multiple varieties of bias identified through the iterations of scientific practice including observer bias and other experimental effects that occur when researchers’ expectations influence study and data outcomes (Holman et.al. 2015 &amp; Young 2009). Biases may be influenced by the following: Researchers expect or assume specific occurrences Research design encourages human subject or researchers to preferentially detect or focus on and recall outcomes that affirm beliefs Analysis or data recording that requires subjective judgements Incentives and agendas or conflicts of interest The effects of bias pervade multiple stages of formal studies and primary data, and so this bias can also affect informal studies, secondary and tertiary data of all sizes (Young 2009). Some of the traditional approaches to control bias and improve credibility include the use of blinding, randomised sampling and peer review. Peer review can be considered the bedrock of credibility for formal studies (Wheeler 2011). Since peer review relies on willing participation between academics to critically assess studies, there are limitations to its power. Hence, peer reviews can also be subject to some biases and conflicts of interest. Many organisations recognize that data in the hands of a few data experts can be powerful, and are hopeful that data at the fingertips of many more domain experts and other staff members will be truly revolutionary, improving knowledge output, efficiency, flexibility or quality of work (Kitchin 2014) The management and interpretation of data through a community of users has the potential to crowdsource insights in a new dynamic that can be likened to peer review. For this reason, the power of self-serve analytics when competently governed and supported may prove more efficient and enriching for the development of industry knowledge than previous infrastructures have allowed. Self-service data analytics models provide a new means to conduct collaboration and peer review. Especially if the functionality to collaborate on understanding data is integrated into the user interface, the review of insights may occur as they are developed. With the current dashboard solutions and reporting mechanisms, the contesting of information is more formalised and structured. A collaborative environment produced by a well implemented self-service analytics strategy has the potential to create collaborative support from peers and mentors that both empowers users and facilitates user learning experiences, improving ways of justifying decisions and developing unique results in a domain (Chesler et. al 2013). 4.2.0.2 META DATA AND DATA CONTEXTS The data does not speak for itself and people are a large component to the production of data driven knowledge. The perception that data is objective has pervaded into industry due to much of the scientific work conducted with “small data” (Kitchin 2011, Michener 2009). However, imperfections in “small data” research have been historically identified as “artifacts”, errors in results or manmade imperfections that distort the properties of the subjects (Schmidt &amp; Hunter 2015). “Data do not exist independently of ideas, techniques, systems, people and contexts, regardless of them often being presented in this manner.” (Kitchin 2014) Although data may have been data widely thought of as benign “raw” elements, which have been abstracted from the world neutrally and objectively, there are many claims to suggest otherwise (Kitchin 2014, Michener 2009). Data are described from established normative, political and ethical processes where decisions about generalisations, assumptions and representations as well as what remains visible and invisible have consequences on the subsequent analysis and conclusions (Kitchin 2014). If data is so socially constructed and ideologically loaded from its conception, then ignoring these contextual aspects about the data risk misinterpretation and misjudgement. Not only this but the storage and sharing of the data becomes problematic if these artefacts from the data are not also passed on (Zimmermann 2008 &amp; Bowker 2000). Unfortunately, the tidy formats that data are transferred in and stored (such as within databases) may fail to maintain the important metadata and information regarding the original agendas of the data (Kitchin 2014). Furthermore, much of repurposed information may not have been maintained to a standard that ensures data artefacts are shared (Zimmermann 2008, Michener 2009). The data can thus become uncoupled from its original political and social contexts leaving only what the organisational rules, philosophies and practices determine to be important (Kitchin 2014). The use of data formats and advances in database and storage options continue to allow for more and more unstructured and unprocessed forms of data to be stored (Song &amp; Zhu 2016, Kitchin 2014, &amp; Service 2017). For instance, unlike traditional data repositories, a data lake is a store of unformatted data where pathways and processes are required to explore the data, since most organisations contain multiple applications with variable non-combined formats (McKennar 2016). Yet throwing all data formats into a ‘data-lake’ may not be the best nor gentlest approach to meeting the thirst for data. Whilst the data stored in data lakes may comply with currently accepted data standards, there is often still a lack of documentation and commonality in standardisation, especially when data is retained from research that has previously been informally stored (McKennar 2016, Kitchin 2014). Much of the data used to develop knowledge in the past has been lost in favour of aggregations or when personal move on, where only the most valuable datasets of cultural and political significance have been retained in data archives (Michener 2009). Again, the uncoupling of data from its context can occur when data is not accountably curated and archived. Secondly, tidy data has quality control, productivity and sense-making advantages; all are vital components to efficiently yielding knowledge from data. Similarly, untidy data is far more difficult to manipulate and interpret for new unfamiliar user groups. The handover of data artefacts therefore does not have a generic solution and will depend on the capabilities and judgement of governance bodies as well as the availability of documentation. In some cases, the quality of data may be compromised such that it becomes stale and unusable due to pore maintenance or the nature of business operations. For instance, through the nature of operations, application data may not enter the data base offered to users for analysis for several months after entering the business pipeline. Such data is not yet digital, whether or not the database can be updated at regular intervals. The data may eventually join the database but the time taken to process it may skew ongoing trends expressed within visualisations. To prevent stale data, fresh and relevant input to these systems require constant maintenance (Marr 2017). Combining large data segments within data can also exhibit Simpson’s Paradox where overall patterns do not reflect the true trends of the separated groups (Huber 2011). It is of great importance that such issues are not overlooked to allow for smooth and correct interactions with data. 4.2.0.3 NON-TECHNICAL USER TRAINING Simply supplying new user groups with access to data and technology will not guarantee success. Just as a person who is illiterate, cannot gain from a rich library of books and written information, so lack of data literacy and experience with interpretation can prevent the use and value extraction from unprepared user groups (Harris 2012, Dykes 2017). Acknowledging gaps in human centred processes and building the confidence and skills to master the self-service systems will ultimately require the bestowal of knowledge and mentorship from those who have a history with the data and with using data tools. Brynjolfssen and McAfee in ‘The Second Machine Age’ predict that the exponential gains expected from combinatorial innovation are intended outcomes of serving data to a wider audience (Brynjolfsson &amp; McAfee 2016). The catch for self-service analytics lies in the scalability of supplying the needed mentorship and training to an ever-extending user group. Even if data could be justified as neutral, the use of data in analysis to develop knowledge, insights and innovation can also become twisted by political and ideological agendas (Young 2009). When data is used to produce knowledge, meaning is derived from complex cognitive processes to form the basis of understanding, explaining and actioning insights (Young 2009). This data analysis stage is human-centred and subjective, with each data consumer framing data from personal knowledge, understanding and experience. For half a century data analysis has been framed to emphasise the application of judgement rather than simply applying mathematical and statistical tools (Tukey 1962). Tukey’s influential paper elaborates that this judgement is constituted by: Subject matter experience Broad experience of analytical tools and techniques applied to various situations As well as judgement of the obtained abstract results. For today’s consumers of data, the business user and analyst alike, the strengths of these components vary greatly. Self-serve analytics and data democratisation shifts the problems of overly technical and potentially irrelevant reporting from more technically experienced and smaller teams to broader groups of people with potential greater perceptions of the data driven business needs but lack of experience utilising data and its insights. Although the potential of discovery and the productivity of data driven knowledge acquisition may have been amplified in the new analytical climate, there is little evidence to suggest that the value can be attained without the proper preparation of new user groups. 4.2.0.4 SUPPORT RESOURCES Data analysis has traditionally been one of the most demanding applications of interactive computing since it covers a wide range of tasks and outputs from research to business intelligence reporting (Huber 2011). Languages and tools for analysis have aimed to be both interactive but also programmable to ensure evidence derived from data is repeatable but also customisable as data requirements change (Huber 2011). As a result, data analytics practices have been inaccessible to the technically untrained. However, the widespread use of computing, and the introduction of more natural language programming languages have opened the opportunities for people to become familiar with data manipulation techniques for a lower overhead. Huber suggests data analysis for novices should be offered in canned form for routine investigations with more flexible methods and customisations available for deeper research (Huber 2011). Dashboards have been the bread and butter approach for providing users with canned visualisations of data for daily use. Where a dashboard’s weaknesses lie such as regarding explanations of the visualisations, updates in the form of reports have been used as a supplement. New vendors such as IBM Watson, Hyper Anna and Data Robot are attempting to hybridise the two approaches so that more customised and complicated analysis can be facilitated by a search sequence. Accessing customised analytics via a search sequence, removes the user’s requirement to know and understand code, and opens the data up to new audiences. This new approach introduces new concerns regarding the unknown levels and types of user support required to ensure automated complex modelling are accountably used and understood. Masking complex data processes behind more user-friendly interfaces is a necessary evolution of these self-serve systems. The consequence is again a lower overhead for training and usability. However, for the user groups that have no ability to investigate the artificial mechanisms and data pipelines behind the scenes, there is a gap in their capacity for data discernment. Without catered and mindful support and mediation of users with the interfaces of these technologies, the quality of interactions is undermined. 4.2.0.5 MISSING PRACTICALITIES FOR TRAINING “Few academics and organizations willingly scrutinize the processes on which we stake so many of our goods and values. Transparency, confidentiality, gatekeeping, resource allocation, institutional reputations for excellence-all inform our vision of ourselves as fairminded, sound, disinterested critics and inhibit self- reflection.” (Wheeler 2011) As data handling is extrapolated to new audiences, previously unfamiliar with the methods of analysis, the requirement for training will increase with each user. Comprehensive and scalable training is therefore currently lacking. At present, general tutorials from third parties are available from vendors, however this is not enough to ensure responsible data management. It cannot be assumed that new audiences will have the necessary time resources, information structures nor motivation to conduct comprehensive self-directed study to understand the data. Since organisations often operate in private and closed data ecosystems, the support for the use of data may need to be facilitated internally (Floridi 2006). Providing support to new users on a large scale is most likely to be solved with virtual solutions. Virtual support with structures like a massive open online course (MOOC) or preferably a massively adaptive complex online simulation (MACROSIM) may provide the information and mentorship infrastructures required (Virtual Internships). Such virtual programs have successfully incorporated a collaborative environment based on learning theories, and encouraged motivation and reflection on action (Chesler et. al 2013, Virtual Internships). Improvements for user support in this new frontier will demand input from users. This will likely occur both anecdotally and through activity feeds where the analytics tools may even be used to process data from the processor (Floridi 2006). Data analytics of this cyclical kind will ultimately change or mutate the entire practice (Kitchin 2014). With the bridge between truly self-directed use and guided exploration through mentorship still open, it is my opinion that data experts will still play a role as data gatekeepers in the near future. The influence of such gatekeepers is yet to be fully explored (Leahey 2008). 4.2.0.5.1 References Bowker, G.C. (2000) Biodiversity Datadiversity, Social Studies of Science, SAGE Publications Ltd; 30(5) 643-683 Brynjolfsson, E., &amp; McAfee A., (2016) The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies, New York W. W. Norton &amp; Company, London Chesler, N.C., Arastoopour, G., D’Angelo, C.M., Bagley, E.A. and Williamson Shaffer, D. (2013) Design of a Professional Practice Simulator for Educating and Motivating First-Year Engineering Students, Advances in Engineering Education, American Society for Engineering Education, Madison Dykes, B. (2016) Self-Service Analytics and the Illusion of Self-Sufficiency, last viewed 6 Nov 2017, https://www.forbes.com/sites/brentdykes/2016/11/15/self-service-analytics-and-the-illusion-of-self-sufficiency/#70349a54219a Dykes, B. (2017) Why Companies Must Close The Data Literacy Divide, Forbes, last viewed 6 Nov 2017, https://www.forbes.com/sites/brentdykes/2017/03/09/why-companies-must-close-the-data-literacy-divide/#6c580d8a369d Harris, J. (2012) Data Is Useless Without the Skills to Analyze It, Harvard Business Review, last viewed 6 Nov 2017, https://hbr.org/2012/09/data-is-useless-without-the-skills Holman, L., Head, M. L., Lanfear, R., and Jennions, M.D. (2015) Evidence of Experimental Bias in the Life Sciences: Why We Need Blind Data Recording, PLoS Biology 13(7) Huber, P. J. (2011) Data analysis: what can be learned from the past 50 years, Wiley series in probability and statistics. Wiley, Hoboken, N.J. Kitchin, R. (2014) The Data Revolution: big data, open data, data infrastructures &amp; their consequences., Sage Publications Ptld, London Leahey, E. (2008) Overseeing Research Practice: The Case of Data Editing, Science, Technology &amp; Human Values, SAGE Publications Inc., vol. 33, no. 5, pp. 620 Lumen Evidence Based Decision Making, last viewed 6 Nov 2017, https://courses.lumenlearning.com/wm-principlesofmanagement/chapter/evidence-based-decision-making/ Marr, B. (2017) What is data democratisation, a super simple explanation and the key pros and cons, last viewed 6 Nov 2017, https://www.forbes.com/sites/bernardmarr/2017/07/24/what-is-data-democratization-a-super-simple-explanation-and-the-key-pros-and-cons/#79ae1ce06013 Michener, William K., and Brunt, James W., eds. (2009) Ecological Data: Design, Management and Processing, Hoboken, GB: Wiley-Blackwell, p92-100 McKennar, B. (2016) Data democratization in the age of big data: why data lakes won’t work, last viewed 6 Nov 2017, http://www.computerweekly.com/blog/Data-Matters/Data-democratization-in-the-age-of-big-data-why-data-lakes-wont-work Rouse, M. (2014) Self-service analytics, Tech Target, last viewed 6 Nov 2017, http://searchbusinessanalytics.techtarget.com/definition/self-service-analytics Service, R. (2017) DNA could store all of the world’s data in one room, last viewed 10 Nov 2017, http://www.sciencemag.org/news/2017/03/dna-could-store-all-worlds-data-one-room Schmidt, F. &amp; Hunter, J. (2015) Methods of meta-analysis, Availability bias, source bias, and publication bias in meta-analysis, SAGE Publications Ltd., London, pp. 513-551 Tukey, J. W. (1962), The Future of Data Analysis, The Annals of Mathemati- cal Statistics, vol. 33, pp. 1-67 Virtual Internships, About, University of Wisconsin-Madison, last viewed 13 Nov 2017, http://virtualinterns.org/about/ Wheeler, B. (2011) The Ontology of the Scholarly Journal and the Place of Peer Review, Journal of Scholarly Publishing, vol. 42, no. 3, pp. 307-322 Young, S. N. (2009) Bias in the research literature and conflict of interest: an issue for publishers, editors, reviewers and authors, and it is not just about the money, Journal of Psychiatry and Neuroscience; vol. 34, no. 6 pp. 412-417 Zimmerman, A. S. (2008) New Knowledge from Old Data: The Role of Standards in the Sharing and Reuse of Ecological Data, Science, Technology, &amp; Human Values, SAGE Publications Inc., vol. 33, no. 5, pp. 631-652 4.2.0.5.2 Bibliography The following articles were not required for writing this post but were influential and complementary readings, I recommend exploring these should you have the interest. Dudley, I. (2016) UNCOMMON SENSE: THE DEMOCRATIZATION OF DATA ANALYSIS, Neilsen Insights last viewed 13 Nov 2017, http://www.nielsen.com/au/en/insights/news/2016/uncommon-sense-the-democratization-of-data-analysis.html Mallows, C. (2006) Tukey’s Paper After 40 Years, Technometrics, American Statistical Association and the American Society for Quality, vol. 48, no. 3 Marr, B. (2017) Why Data Democratization Is Such a Game-Changer In Our Big Data World, last viewed 6 Nov 2017, http://data-informed.com/why-data-democratization-is-such-a-game-changer-in-our-big-data-world/ Moats, D. (2015) Review of Rob Kitchin’s The Data Revolution, last viewed 6 Nov 2017, https://www.theoryculturesociety.org/review-of-rob-kitchins-the-data-revolution/ Dykes, B. (2015) The Age Of Data Democratization: How To Effectively Share Data Across Your Business, last viewed 6 Nov 2017, https://www.forbes.com/sites/brentdykes/2015/09/09/the-age-of-data-democratization-how-to-effectively-share-data-across-your-business/#261201ac6c50 Strom, D &amp; Baker, P. (2017) The Best Self-Service Business Intelligence (BI) Tools of 2017, last viewed 6 Nov 2017, http://au.pcmag.com/cloud-services/41015/guide/the-best-self-service-business-intelligence-bi-tools-of-2017 Kitchin, R. (2014) Rob Kitchin talks about big data, open data and the ‘data revolution’, Sage Publications Inc., last viewed 6 Nov 2017, https://www.youtube.com/watch?v=QpDfLoUHqE4 Shah HM, Chung KC. Archie Cochrane and his vision for evidence-based medicine. Plastic and reconstructive surgery. 2009;124(3):982-988. doi:10.1097/PRS.0b013e3181b03928. last viewed 6 Nov 2017, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2746659/ On decision making cultures see: https://hbr.org/2012/10/big-data-the-management-revolution 4.3 Herry 4.4 Passiona 4.5 Rory 4.6 Tracy "],
["contributors-to-fliparound.html", "Chapter 5 Contributors to FlipAround 5.1 If you want to contribute", " Chapter 5 Contributors to FlipAround What Who.did.it.for.you FlipAround Team Zherish Opperman; Detlev Kerkovius; Rory Angus; Dorotea Baljevic; Herry Basuki Editorial Zherish Opperman; Detlev Kerkovius Content Zherish Opperman; Detlev Kerkovius; Amela Peric; Theresa Anderson; Andrew Waites; Corinna Mittmann Layout Zherish Opperman; Detlev Kerkovius; Amela Peric; Theresa Anderson Integrations Perry Stephenson 5.1 If you want to contribute Go to the GitHub repo for more information…or follow the dummies guide below. 5.1.1 Install GIT on your computer Easiest is to just google a way but this link will probably get you there and it is worth following the advice on telling Git about your keys and such so you don’t spend your life typing in passwords. 5.1.2 Create a Github Account If you already have an account then ignore this, otherwise go to https://github.com/ and sign up for an account. You might want to use your UTS account - this may make it easier to pick up the GIT Student Pack 5.1.3 Make a Fork This is your own little copy of the fliparound sources - you can play to your heart’s content with this without stuffing anyone up. To do this, log on to github then go to https://github.com/FlipAround/book which should look a bit like the image below - to make a fork just click on the icon indicated (I had to avoid putting it into my corporate repo but if you are new to Git it shouldn’t ask too many more questions - if it does, make a note and update this text once you have worked it out.) Once the fork is done you will get a screen that looks much like the normal fliparound screen but the name on the repository (repo) will now be yours…. (see andrewwaites/book). Now you want to clone this so you have a local copy…see arrow. On this screen that little icon will copy the link you need to do the clone on your own system. 5.1.4 Get Local So now you can open a terminal window on your system and make a local copy of all the fliparound files in your favourite directory (aka folder) using “git clone” + Cmd+V (Paste the URL you just copied) - note that I am using a Mac (sorry if you are using Windows - very). Also, while you are there, create a new branch to work in - here called “andrew” cd ~/Dropbox/Development git clone https://github.com/andrewwaites/book.git git checkout -b andrew This should create a directory called “book” under your current directory. If you cd into that directory it will look a bit like this….note that the (andrew) in the prompt tells me what branch I am currently working in. iMac:book: (andrew) &gt; ls -la total 312 drwxr-xr-x@ 32 andrewwaites staff 1088 13 Nov 13:53 . drwxr-xr-x@ 25 andrewwaites staff 850 12 Nov 19:51 .. -rw-r--r--@ 1 andrewwaites staff 6148 13 Nov 12:07 .DS_Store -rw-r--r--@ 1 andrewwaites staff 28 7 Nov 21:34 .Rbuildignore -rw-r--r--@ 1 andrewwaites staff 0 7 Nov 21:33 .Rhistory drwxr-xr-x@ 4 andrewwaites staff 136 7 Nov 21:34 .Rproj.user drwxr-xr-x@ 15 andrewwaites staff 510 13 Nov 13:55 .git -rw-r--r--@ 1 andrewwaites staff 40 7 Nov 21:34 .gitignore -rw-r--r--@ 1 andrewwaites staff 218 7 Nov 21:32 .travis.yml -rw-r--r--@ 1 andrewwaites staff 11067 7 Nov 21:32 01-DSmind.Rmd -rw-r--r--@ 1 andrewwaites staff 32513 12 Nov 11:42 02-SurvivalGuide.Rmd -rw-r--r--@ 1 andrewwaites staff 13661 12 Nov 11:47 03-DataFutures.Rmd -rw-r--r--@ 1 andrewwaites staff 2012 13 Nov 13:53 04-Contributors.Rmd -rw-r--r--@ 1 andrewwaites staff 315 7 Nov 21:32 Contributors.csv -rw-r--r--@ 1 andrewwaites staff 72 7 Nov 21:32 DESCRIPTION -rw-r--r--@ 1 andrewwaites staff 277 7 Nov 21:32 FlipAround.Rproj drwxr-xr-x@ 42 andrewwaites staff 1428 13 Nov 12:14 Images -rw-r--r--@ 1 andrewwaites staff 6556 7 Nov 21:32 LICENSE -rw-r--r--@ 1 andrewwaites staff 1073 7 Nov 21:32 MDSIJargon.csv -rw-r--r--@ 1 andrewwaites staff 1431 7 Nov 21:32 README.md drwxr-xr-x@ 11 andrewwaites staff 374 13 Nov 13:53 _book -rw-r--r--@ 1 andrewwaites staff 67 7 Nov 21:32 _bookdown.yml -rw-r--r--@ 1 andrewwaites staff 80 7 Nov 21:32 _build.sh -rw-r--r--@ 1 andrewwaites staff 418 7 Nov 21:32 _deploy.sh -rw-r--r--@ 1 andrewwaites staff 1098 7 Nov 21:32 _output.yml -rw-r--r--@ 1 andrewwaites staff 303 7 Nov 21:34 book.Rproj -rw-r--r--@ 1 andrewwaites staff 268 7 Nov 21:32 book.bib -rw-r--r--@ 1 andrewwaites staff 3428 12 Nov 11:43 index.Rmd -rw-r--r--@ 1 andrewwaites staff 1053 7 Nov 21:32 packages.bib -rw-r--r--@ 1 andrewwaites staff 161 7 Nov 21:32 preamble.tex -rw-r--r--@ 1 andrewwaites staff 172 7 Nov 21:32 style.css -rw-r--r--@ 1 andrewwaites staff 2443 7 Nov 21:32 toc.css 5.1.5 Do Stuff You can now open up R-Studio, set your working directory to this directory and make edits to the various .Rmd files (how to do markdown is a whole other discussion). Make sure that you put any images in the /Images subdirectory. Use the existing Rmd files as templates to show you how to do stuff. So you will probably edit some existing files and maybe create some new image files. 5.1.6 Commit and Push When you are happy with your work you are ready to send it back up the line. To do that you need to commit your changes, push them from your local machine to your fork on github then ask a friendly fliparound master (currently Detlev) to merge them in to the main book for you. Let’s start with those first 2 steps. You need to identify what files you want to send in. Git will show you what files you have touched (in any way) to give you a hint. Use git status to see where things stand. iMac:book: (andrew) &gt; git status On branch andrew Your branch is up-to-date with &#39;origin/andrew&#39;. Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: 04-Contributors.Rmd Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .Rbuildignore .gitignore Images/AfterFork.png Images/BookClone.png Images/GitFork.png _book/ book.Rproj no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) Before anything can be committed you need to stage files for the commit by git add. Lets add the 04-Contributors.Rmd (this file) and some images. iMac:book: (andrew) &gt; git add 04-Contributors.Rmd iMac:book: (andrew) &gt; git add Images/AfterFork.png Images/BookClone.png Images/GitFork.png iMac:book: (andrew) &gt; git status On branch andrew Your branch is up-to-date with &#39;origin/andrew&#39;. Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: 04-Contributors.Rmd new file: Images/AfterFork.png new file: Images/BookClone.png new file: Images/GitFork.png Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .Rbuildignore .gitignore _book/ book.Rproj Now markdown is not being quite as helpful as git because on your screen those Changes to be committed would be in green and the other unstaged files would be in red. You can ignore all those files shown above as untracked. Now you need to commit your staged changes which will possibly throw you into some weird command line editor unless you add a message to the command line (really good idea if you don’t want to spend 20 minutes googling how to exit an EMACS editor). Then you can push that commit from your local branch to the remote (central) repo and check out the status. iMac:book: (andrew) &gt; git commit -m &quot;A first cut at a dummies guide to changing fliparound via git repo...&quot; [andrew ff9b185] A first cut at a dummies guide to changing fliparound via git repo... 4 files changed, 157 insertions(+), 1 deletion(-) create mode 100644 Images/AfterFork.png create mode 100644 Images/BookClone.png create mode 100644 Images/GitFork.png iMac:book: (andrew) &gt; git push origin andrew Counting objects: 7, done. Delta compression using up to 8 threads. Compressing objects: 100% (7/7), done. Writing objects: 100% (7/7), 739.86 KiB | 30.83 MiB/s, done. Total 7 (delta 2), reused 0 (delta 0) remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To https://github.com/andrewwaites/book.git 631724a..ff9b185 andrew -&gt; andrew Time to go back to your browser and submit a pull request to have your changes merged into the big book. 5.1.7 Submit a Pull Request So if you go to your github copy of all this stuff https://github.com/andrewwaites/book/pulls (well yours might have something different where this link says andrewwaites like your github username) then click on the big green button that says New Pull Request. Figure 5.1: Wrong Branch When you go to the page you will probably not see any changes to submit - don’t panic this is likely because you are on the master branch. Just change that right hand dropdown (beside your fork) to the name of the branch you created (in this case, andrew). Figure 5.2: Start Pull Request So now you should see a bunch of changes that you have made that you want to submit. If you press that Create Pull Request button again you will get an opportunity to put some comments in and the like…. Figure 5.3: Comments and Such Once you fill out any comments and press the Submit Pull Request button again you will get a screen notifying you that the request has gone in but no further progress will be made until it is all accepted. Figure 5.4: Pending Review 5.1.8 Now You Wait All out of your hands now. Git will email you some updates as the review process progresses and once your changes have been merged and the deploy effected you will see your work appear in fliparound - congratulations and thanks for your efforts. "]
]
